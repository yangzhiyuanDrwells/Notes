
<font size = 4 face = "黑体">

<a href="#基本概念">基本概念</a>
<a href="#五种IO模型">五种IO模型</a>

<a id="基本概念"></a>

<br/><br/>

# 基本概念
&emsp;&emsp;<a href="#syn_Communication">同步通信</a>

&emsp;&emsp;<a href="#asynchronism_Communication">异步通信</a>

&emsp;&emsp;<a href="#block_vs_unblock">阻塞 vs 非阻塞</a>

<a id="syn_Communication"></a>

<br/><br/>


## 同步通信^[synchronous communication]、异步通信^[asynchronous communication]

同步和异步关注的是消息通信机制

### 同步通信

所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了;换句话说，就是由调用者主动等待这个调用的结果;

<a id="asynchronism_Communication"></a>

<br/><br/>


## 异步通信

调用在发出之后，这个调用就直接返回了，所以没有返回结果;换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果;而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用.

<a id="block_vs_unblock"></a>

<br/><br/>


## 阻塞 vs 非阻塞

阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.

阻塞调用是指调用结果返回之前，当前线程会被挂起. 调用线程只有在得到结果之后才会返回.

非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程.


<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=50>













<a id="五种IO模型"></a>

<br/><br/>


# 五种IO模型

&emsp;&emsp;<a href="#block_IO">阻塞IO</a>

&emsp;&emsp;<a href="#unblock_IO">非阻塞IO</a>

&emsp;&emsp;<a href="#signal_drive_IO">信号驱动IO</a>

&emsp;&emsp;<a href="#IO_Multiplexing">IO多路转接复用</a>

&emsp;&emsp;&emsp;&emsp;<a href="#select">select</a>

&emsp;&emsp;&emsp;&emsp;<a href="#poll">poll</a>

&emsp;&emsp;&emsp;&emsp;<a href="#epoll">epoll</a>

&emsp;&emsp;<a href="#asynchronism_IO">异步IO</a>


任何IO过程中, 都包含两个步骤. 第一是等待, 第二是拷贝. 而且在实际的应用场景中, 等待消耗的时间往往都远远高于拷贝的时间. 让IO更高效, 最核心的办法就是让等待的时间尽量少

<a id="block_IO"></a>

<br/><br/>


## 阻塞IO


在内核将数据准备好之前, 系统调用会一直等待。 所有的套接字, 默认都是阻塞方式;一个文件描述符, 默认都是阻塞IO

**在应用调用recvfrom读取数据时，其系统调用直到数据包到达且被复制到应用缓冲区中或者发送错误时才返回，在此期间一直会等待，进程从调用到返回这段时间内都是被阻塞的称为阻塞IO**


##### 阻塞读文件描述符


```
#include <stdio.h>
#include <errno.h> //errno
#include <unistd.h>
int main() {
    errno = 0;
    while (1) {
        char buf[1024] = { 0 };
        ssize_t read_size = read(0, buf, sizeof(buf) - 1); //阻塞读取
        printf("start reading...\n");
        printf("input:%s\n", buf);
    }
    return 0;
}
```

程序执行就会被阻塞在read，因为标准输入没有数据，一旦标准输入数据准备好，read才会继续执行


<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=10>
<a id="unblock_IO"></a>

<br/><br/>



## 非阻塞IO

如果内核还未将数据准备好, 系统调用仍然会直接返回, 并且返回EWOULDBLOCK错误码.

非阻塞IO往往需要程序员循环的方式反复尝试读写文件描述符, 这个过程称为轮询. 这对CPU来说是较大的浪费, 一般只有特定场景下才使用

**非阻塞IO是在应用调用recvfrom读取数据时，如果该缓冲区没有数据的话，就会直接返回一个EWOULDBLOCK错误，不会让应用一直等待中。在没有数据的时候会即刻返回错误标识，那也意味着如果应用要读取数据就需要不断的调用recvfrom请求，直到读取到它数据要的数据为止。**



### fcntl函数


> #include <unistd.h>
> #include <fcntl.h>

#### 函数原型

    int fcntl(int fd, int cmd, ... /* arg */ );
    

#### 参数及返回值

fd：文件描述符

传入的cmd的值不同, 后面追加的参数也不相同


#### 功能


- 复制一个现有的描述符（cmd=F_DUPFD）.

- 获得/设置文件描述符标记(cmd=F_GETFD或F_SETFD).

- 获得/设置文件状态标记(cmd=F_GETFL或F_SETFL).

- 获得/设置异步I/O所有权(cmd=F_GETOWN或F_SETOWN).

- 获得/设置记录锁(cmd=F_GETLK,F_SETLK或F_SETLKW).



最重要最常用的就是**获取/设置文件状态标记**, 可以将一个文件描述符设置为非阻塞


##### 将文件描述符设置为非阻塞

基于fcntl, 我们实现一个nonBlock函数,将文件描述符设置为非阻塞


```
void nonBlock(int fd) {
	int fl = fcntl(fd, F_GETFL);
	if (fl < 0) {
		perror("fcntl");
		return;
	}
	fcntl(fd, F_SETFL, fl | O_NONBLOCK);
}
```

使用F_GETFL将当前的文件描述符的属性取出来(这是一个位图)\

然后再使用F_SETFL将文件描述符设置回去. 设置回去的同时, 加上一个O_NONBLOCK参数.


##### 非阻塞轮询读文件描述符(读取标准输入)


###### 标准输入没消息read返回并按错误处理


```
#include <stdio.h>
#include <unistd.h>
#include <fcntl.h>
void SetNoBlock(int fd) {
	int fl = fcntl(fd, F_GETFL);
	if (fl < 0) {
		perror("fcntl");
		return;
	}
	fcntl(fd, F_SETFL, fl | O_NONBLOCK);
}
int main() {
	SetNoBlock(0);
	while (1) {
		char buf[1024] = { 0 };
		ssize_t read_size = read(0, buf, sizeof(buf) - 1);
		if (read_size < 0) {
			perror("read");  //按出错处理
			sleep(1);
			continue;
		}
		printf("input:%s\n", buf);
	}
	return 0;
}
```
证明执行到read不会阻塞，会继续执行

###### 标准输入没消息read返回并区分errno

```
#include <stdio.h>
#include <errno.h> //errno
#include <unistd.h>
#include <fcntl.h>
void SetNoBlock(int fd) {
    int fl = fcntl(fd, F_GETFL);
    if (fl < 0) {
        perror("fcntl");
        return;
    }
    fcntl(fd, F_SETFL, fl | O_NONBLOCK);
}
int main() {
    errno = 0;
    SetNoBlock(0);
    while (1) {
        char buf[1024] = { 0 };
        ssize_t read_size = read(0, buf, sizeof(buf) - 1); //非阻塞直接返回
        if( errno == EAGAIN) //EAGAIN正常情况，表示缓冲区数据未准备好
            continue;
        if (read_size < 0) {
            perror("read");
            sleep(1);
            continue;
        }
        printf("input:%s\n", buf);
    }
    return 0;
}
```

read系统调用返回，errno是EAGAIN，表示正常情况

缓冲区数据未准备好，read以轮询方式尝试读

<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=10>



<a id="signal_drive_IO"></a>

<br/><br/>


## 信号驱动IO^[内核将数据准备好的时候, 使用SIGIO信号通知应用程序进行IO操作]

复用IO模型解决了一个线程可以监控多个fd的问题，但是select是采用轮询的方式来监控多个fd的，通过不断的轮询fd的可读状态来知道是否就可读的数据，大部分情况下的轮询都是无效的，换成等内核数据准备好了就通知的方式，所以就衍生了信号驱动IO模型。


信号驱动IO在调用sigaction时候建立一个SIGIO的信号联系，当内核数据准备好之后再通过SIGIO信号通知线程数据准备好后的可读状态，当线程收到可读状态的信号后，此时再向内核发起recvfrom读取数据的请求，因为信号驱动IO的模型下应用线程在发出信号监控后即可返回，不会阻塞，所以这样的方式下，一个应用线程也可以同时监控多个fd。

**首先开启套接口信号驱动IO功能，并通过系统调用sigaction执行一个信号处理函数，此时请求即刻返回，当数据准备就绪时，就生成对应进程的SIGIO信号，通过信号回调通知应用线程调用recvfrom来读取数据。**

<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=10>
<a id="IO_Multiplexing"></a>

<br/><br/>


## IO多路转接复用^[IO多路转接能够同时等待多个文件描述符的就绪状态]

并发情况下服务器很可能一瞬间会收到几十上百万的请求，这种情况下应用B就需要创建几十上百万的线程去读取数据，同时又因为应用线程是不知道什么时候会有数据读取，为了保证消息能及时读取到，那么这些线程自己必须不断的向内核发送recvfrom 请求来读取数据；显然太浪费性能。

IO复用模型的思路就是系统提供了一种函数可以同时监控多个fd的操作，这个函数就是我们常说到的select、poll、epoll函数。虽然从流程图上看起来和阻塞IO类似. 实际上最核心在于IO多路转接能够同时等待多个文件描述符的就绪状态。

**进程通过将一个或多个fd传递给select，阻塞在select操作上，select帮我们侦测多个fd是否准备就绪，当有fd准备就绪时，select返回数据可读状态，应用程序再调用recvfrom读取数据**。复用IO的基本思路就是通过slect或poll、epoll来监控多fd，来达到不必为每个fd创建一个对应的监控线程，从而减少线程资源创建的目的。

<a id="select"></a>

<br/><br/>


### select

> #include <sys/select.h>


#### 函数原型

    int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
    
#### 参数及返回值


##### 参数

nfds：是需要监视的最大的文件描述符值+1；

readfds,writefds,exceptfds：分别对应于需要检测的可读文件描述符的集合，可写文件描述符的集 合及异常文件描述符的集合;

timeout：为结构timeval，用来设置select()的等待时间
-   NULL：则表示select（）没有timeout，select将一直被阻塞，直到某个文件描述符上发生了事件;
-   0：仅检测描述符集合的状态，然后立即返回，并不等待外部事件的发生。
-   特定的时间值：如果在指定的时间段里没有事件发生，select将超时返回。

##### 返回值

On  success,  select()  and pselect() return the number of file descriptors contained in the three returned descriptor sets (that is, the total number of bits that are set in readfds, writefds, exceptfds) which may be zero if the timeout expires  before  anything interesting happens.  On error, -1 is returned, and errno is set appropriately; the sets and timeout become undefined, so do not rely on their contents after an error.

成功返回文件描述符集的数量，时间超限返回0，错误返回-1，并设置errno


###### fd_set结构

- 查找fd_set

    grep -rn "fd_set" --include="*.h" /usr/include/sys/select.h
    
> 位于select.h 75行之前

- cat查看fd_set

    cat -n select.h

```
    ...
    53	/* The fd_set member is required to be an array of longs.  */
    54	typedef long int __fd_mask;
    55	
    ...
    64	typedef struct
    65	  {
    66	    /* XPG4.2 requires this member name.  Otherwise avoid the name
    67	       from the global namespace.  */
    68	#ifdef __USE_XOPEN
    69	    __fd_mask fds_bits[__FD_SETSIZE / __NFDBITS];
    70	# define __FDS_BITS(set) ((set)->fds_bits)
    71	#else
    72	    __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS];
    73	# define __FDS_BITS(set) ((set)->__fds_bits)
    74	#endif
    75	  } fd_set;
```

这个结构就是一个整数数组, 更严格的说,是一个"位图".使用位图中对应的位来表示要监视的文件描述符.提供了一组操作fd_set的接口,来比较方便的操作位图


-   void FD_CLR(int fd, fd_set *set); // 用来清除描述词组set中相关fd 的位

-   int FD_ISSET(int fd, fd_set *set); // 用来测试描述词组set中相关fd 的位是否为真

-   void FD_SET(int fd, fd_set *set); // 用来设置描述词组set中相关fd的位

-   void FD_ZERO(fd_set *set); // 用来清除描述词组set的全部位


###### timeval结构

timeval结构用于描述一段时间长度，如果在这个时间内，需要监视的描述符没有事件发生则函数返回，返回值为0

    grep -rn "timeval" /usr/include/linux/time.h //15行
    
    cd /usr/include/linux/
    
    cat -n time.h


```
    ...
    15	struct timeval {
    16		__kernel_time_t		tv_sec;		/* seconds */
    17		__kernel_suseconds_t	tv_usec;	/* microseconds */
    18	};
    ...
```





##### 返回值

执行成功则返回文件描述词状态已改变的个数

如果返回0代表在描述词状态改变前已超过timeout时间

当有错误发生时则返回-1，错误原因存于errno，此时参数readfds，writefds, exceptfds和timeout的值变成不可预测。

错误值可能为：
- EBADF 文件描述词为无效的或该文件已关闭
- EINTR 此调用被信号所中断
- EINVAL 参数n 为负值。
- ENOMEM 核心内存不足


#### 功能


系统提供select函数来实现多路复用输入/输出模型,select系统调用是用来让我们的程序监视多个文件描述符的状态变化的;程序会停在select这里等待，直到被监视的文件描述符有一个或多个发生了状态改变;


#### select执行过程

理解select模型的关键在于理解fd_set,为说明方便，取fd_set长度为1字节，fd_set中的每一bit可以对应一个文件描述符fd。则1字节长的fd_set最大可以对应8个fd.


- （1）执行fd_set set; FD_ZERO(&set);则set用位表示是0000,0000。

- （2）若fd＝5,执行FD_SET(fd,&set);后set变为0001,0000(第5位置为1)

- （3）若再加入fd＝2，fd=1,则set变为0001,0011 

- （4）执行select(6,&set,0,0,0)阻塞等待 

- （5）若fd=1,fd=2上都发生可读事件，则select返回，此时set变为0000,0011。注意：没有事件发生的fd=5被清空

<a id="select-socket"></a>

<br/><br/>



#### select模型下socket就绪条件

。。。



#### select的特点

- 可监控的文件描述符个数取决与sizeof(fd_set)的值. 我这边服务器上sizeof(fd_set)＝512，每bit表示一个文件
描述符，则我服务器上支持的最大文件描述符是512*8=4096.

- 将fd加入select监控集的同时，还要再使用一个数据结构array保存放到select监控集中的fd，
- - 一是用于再select 返回后，array作为源数据和fd_set进行FD_ISSET判断。
- - 二是select返回后会把以前加入的但并无事件发生的fd清空，则每次开始select前都要重新从array取得
fd逐一加入(FD_ZERO最先)，扫描array的同时取得fd最大值maxfd，用于select的第一个参数。

> fd_set的大小可以调整，可能涉及到重新编译内核

#### select缺点

- 每次调用select, 都需要手动设置fd集合, 从接口使用角度来说也非常不便.
- 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
- 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
- select支持的文件描述符数量太小.

#### select使用示例: 检测标准输入输出



```
#include <stdio.h>
#include <unistd.h>
#include <sys/select.h>
int main() {
	fd_set read_fds;
	FD_ZERO(&read_fds);
	FD_SET(0, &read_fds);
	for (;;) {
		printf("> ");
		fflush(stdout);
		int ret = select(1, &read_fds, NULL, NULL, NULL);
		if (ret < 0) {
			perror("select");
			continue;
		}
		if (FD_ISSET(0, &read_fds)) {
			char buf[1024] = { 0 };
			read(0, buf, sizeof(buf) - 1);
			printf("input: %s", buf);
		}
		else {
			printf("error! invaild fd\n");
			continue;
		}
		FD_ZERO(&read_fds);
		FD_SET(0, &read_fds);
	}
	return 0;
}
```

> 当只检测文件描述符0（标准输入）时，因为输入条件只有在你有输入信息的时候，才成立，所以如果一直不输入，就会产生超时信息


#### select使用示例

##### 使用 select 实现字典服务器

> tcp_select_server.hpp


```
#pragma once
#include <vector>
#include <unordered_map>
#include <functional>
#include <sys/select.h>
#include "tcp_socket.hpp"
// 必要的调试函数
inline void PrintFdSet(fd_set* fds, int max_fd) {
	printf("select fds: ");
	for (int i = 0; i < max_fd + 1; ++i) {
		if (!FD_ISSET(i, fds)) {
			continue;
		}
		printf("%d ", i);
	}
	printf("\n");
}
typedef std::function<void(const std::string& req, std::string* resp)> Handler;
// 把 Select 封装成一个类. 这个类虽然保存很多 TcpSocket 对象指针, 但是不管理内存
class Selector {
public:
	Selector() {
		// [注意!] 初始化千万别忘了!!
		max_fd_ = 0;
		FD_ZERO(&read_fds_);
	}
	bool Add(const TcpSocket& sock) {
		int fd = sock.GetFd();
		printf("[Selector::Add] %d\n", fd);
		if (fd_map_.find(fd) != fd_map_.end()) {
			printf("Add failed! fd has in Selector!\n");
			return false;
		}
		fd_map_[fd] = sock;
		FD_SET(fd, &read_fds_);
		if (fd > max_fd_) {
			max_fd_ = fd;
		}
		return true;
	}
	bool Del(const TcpSocket& sock) {
		int fd = sock.GetFd();
		printf("[Selector::Del] %d\n", fd);
		if (fd_map_.find(fd) == fd_map_.end()) {
			printf("Del failed! fd has not in Selector!\n");
			return false;
		}
		fd_map_.erase(fd);
		FD_CLR(fd, &read_fds_);
		// 重新找到最大的文件描述符, 从右往左找比较快
		for (int i = max_fd_; i >= 0; --i) {
			if (!FD_ISSET(i, &read_fds_)) {
				continue;
			}
			max_fd_ = i;
			break;
		}
		return true;
	}
	// 返回读就绪的文件描述符集
	bool Wait(std::vector<TcpSocket>* output) {
		output->clear();
		// [注意] 此处必须要创建一个临时变量, 否则原来的结果会被覆盖掉
		fd_set tmp = read_fds_;
		// DEBUG
		PrintFdSet(&tmp, max_fd_);
		int nfds = select(max_fd_ + 1, &tmp, NULL, NULL, NULL);
		if (nfds < 0) {
			perror("select");
			return false;
		}
		// [注意!] 此处的循环条件必须是 i < max_fd_ + 1
		for (int i = 0; i < max_fd_ + 1; ++i) {
			if (!FD_ISSET(i, &tmp)) {
				continue;
			}
			output->push_back(fd_map_[i]);
		}
		return true;
	}
private:
	fd_set read_fds_;
	int max_fd_;
	// 文件描述符和 socket 对象的映射关系
	std::unordered_map<int, TcpSocket> fd_map_;
};
class TcpSelectServer {
public:
	TcpSelectServer(const std::string& ip, uint16_t port) : ip_(ip), port_(port) {
	}
	bool Start(Handler handler) const {
		// 1. 创建 socket
		TcpSocket listen_sock;
		bool ret = listen_sock.Socket();
		if (!ret) {
			return false;
		}
		// 2. 绑定端口号
		ret = listen_sock.Bind(ip_, port_);
		if (!ret) {
			return false;
		}
		// 3. 进行监听
		ret = listen_sock.Listen(5);
		if (!ret) {
			return false;
		}
		// 4. 创建 Selector 对象
		Selector selector;
		selector.Add(listen_sock);
		// 5. 进入事件循环
		for (;;) {
			std::vector<TcpSocket> output;
			bool ret = selector.Wait(&output);
			if (!ret) {
				continue;
			}
			// 6. 根据就绪的文件描述符的差别, 决定后续的处理逻辑
			for (size_t i = 0; i < output.size(); ++i) {
				if (output[i].GetFd() == listen_sock.GetFd()) {
					// 如果就绪的文件描述符是 listen_sock, 就执行 accept, 并加入到 select 中
					TcpSocket new_sock;
					listen_sock.Accept(&new_sock, NULL, NULL);
					selector.Add(new_sock);
				}
				else {
					// 如果就绪的文件描述符是 new_sock, 就进行一次请求的处理
					std::string req, resp;
					bool ret = output[i].Recv(&req);
					if (!ret) {
						selector.Del(output[i]);
						// [注意!] 需要关闭 socket
						output[i].Close();
						continue;
					}
					// 调用业务函数计算响应
					handler(req, &resp);
					// 将结果写回到客户端
					output[i].Send(resp);
				}
			} // end for
		} // end for (;;)
		return true;
	}
private:
	std::string ip_;
	uint16_t port_;
}
```

##### dict_server.c

这个代码和<a href="https://blog.csdn.net/qq_43808700/article/details/115870749?utm_source=app#dict_server">之前9.网络编程套接字-->dict_server</a>相同, 只是把里面的 server 对象改成 TcpSelectServer 类即可


<a id="poll"></a>

<br/><br/>


### poll

> include <poll.h>


#### 函数原型

    int poll(struct pollfd *fds, nfds_t nfds, int timeout);
    
#### 参数及返回值


##### 参数

###### struct pollfd


```
    struct pollfd {
        int fd; /* file descriptor */
        short events; /* requested events */
        short revents; /* returned events */
    };
```

<a href="https://blog.csdn.net/qq_43808700/article/details/116055848?utm_source=app">events和revents的取值</a>



###### fds

fds是一个poll函数监听的结构列表. 每一个元素中, 包含了三部分内容: 文件描述符, 监听的事件集合, 返回的事件集合.

######  nfds

nfds表示fds数组的长度.

######  timeout

timeout表示poll函数的超时时间, 单位是毫秒(ms).

##### 返回值

- 返回值小于0, 表示出错;
- 返回值等于0, 表示poll函数等待超时;
- 返回值大于0, 表示poll由于监听的文件描述符就绪而返回.

#### poll下socket就绪条件

同<a href="#select-socket">select</a>



#### poll的优点

不同与select使用三个位图来表示三个fdset的方式，poll使用一个pollfd的指针实现.
- pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式. 接口使用比select更方便.
- poll并没有最大数量限制 (但是数量过大后性能也是会下降).

#### poll的缺点


poll中监听的文件描述符数目增多时
- 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符.
- 每次调用poll都需要把大量的pollfd结构从用户态拷贝到内核中.
- 同时连接的大量客户端在一时刻可能只有很少的处于就绪状态, 因此随着监视的描述符数量的增长, 其效率也会线性下降.

#### 使用poll监控标准输入


```
#include <poll.h>
#include <unistd.h>
#include <stdio.h>
int main() {
	struct pollfd poll_fd;
	poll_fd.fd = 0;
	poll_fd.events = POLLIN;
	for (;;) {
		int ret = poll(&poll_fd, 1, 1000);
		if (ret < 0) {
			perror("poll");
			continue;
		}
		if (ret == 0) {
			printf("poll timeout\n");
			continue;
		}
		if (poll_fd.revents == POLLIN) {
			char buf[1024] = { 0 };
			read(0, buf, sizeof(buf) - 1);
			printf("stdin:%s", buf);
		}
	}
}
```


<a id="epoll"></a>

<br/><br/>


### epoll



epoll is a new API introduced in Linux kernel 2.5.44，具备了IO多路转接复用所说的一切优点，被公认为Linux2.6下性能最好的多路I/O就绪通知方法

#### epoll工作原理



##### epoll的使用过程三部曲:


1. 调用epoll_create创建一个epoll句柄;
2. 调用epoll_ctl, 将要监控的文件描述符进行注册;
3. 调用epoll_wait, 等待文件描述符就绪;

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210423150556179.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzODA4NzAw,size_16,color_FFFFFF,t_70)


##### 详细过程

- 当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成
员与epoll的使用方式密切相关.

    struct eventpoll{
        ....
        /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
        struct rb_root rbr;
        /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
        struct list_head rdlist;
        ....
    };
    
- 每一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件.

- 这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插
入时间效率是lgn，其中n为树的高度).

- 而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当响应的事件发生时
会调用这个回调方法.

- 这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中.

- 在epoll中，对于每一个事件，都会建立一个epitem结构体.

    struct epitem{
        struct rb_node rbn;//红黑树节点
        struct list_head rdllink;//双向链表节点
        struct epoll_filefd ffd; //事件句柄信息
        struct eventpoll *ep; //指向其所属的eventpoll对象
        struct epoll_event event; //期待发生的事件类型
    }

- 当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可.

- 如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户. 这个操作的时间复杂度
是O(1).




#### epoll相关函数

##### epoll_create

    int epoll_create(int size);
    
创建一个epoll的句柄

- 自从linux2.6.8之后，size参数是被忽略的
- 用完之后, 必须调用close()关闭


##### epoll_ctl

    int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

epoll的事件注册函数


- 它不同于select()是在监听事件时告诉内核要监听什么类型的事件, 而是在这里先注册要监听的事件类型.
- 第一个参数是epoll_create()的返回值(epoll的句柄).
- 第二个参数表示动作，用三个宏来表示.
- - EPOLL_CTL_ADD ：注册新的fd到epfd中；
- - EPOLL_CTL_MOD ：修改已经注册的fd的监听事件；
- - EPOLL_CTL_DEL ：从epfd中删除一个fd；
- 第三个参数是需要监听的fd.
- 第四个参数是告诉内核需要监听什么事.


###### struct epoll_event

    grep -rn "struct epoll_event" /usr/include/
    
> /usr/include/linux/eventpoll.h:62

    cat -n /usr/include/linux/eventpoll.h
    
    

```
    ...
    62	struct epoll_event {
    63		__u32 events;
    64		__u64 data;
    65	} EPOLL_PACKED;
    ...
```

events可以是以下几个宏的集合：
- EPOLLIN : 表示对应的文件描述符可以读 (包括对端SOCKET正常关闭);
- EPOLLOUT : 表示对应的文件描述符可以写;
- EPOLLPRI : 表示对应的文件描述符有紧急的数据可读 (这里应该表示有带外数据到来);
- EPOLLERR : 表示对应的文件描述符发生错误;
- EPOLLHUP : 表示对应的文件描述符被挂断;
- EPOLLET : 将EPOLL设为边缘触发(Edge Triggered)模式, 这是相对于水平触发(Level Triggered)来说的.
- EPOLLONESHOT：只监听一次事件, 当监听完这次事件之后, 如果还需要继续监听这个socket的话, 需要
再次把这个socket加入到EPOLL队列里.


##### epoll_wait

    int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
    
收集在epoll监控的事件中已经发送的事件

- 参数events是分配好的epoll_event结构体数组.
- epoll将会把发生的事件赋值到events数组中(events不可以是空指针，内核只负责把数据复制到这个events数组中，不会去帮助我们在用户态中分配内存).
- maxevents告之内核这个events有多大，这个 maxevents的值不能大于创建epoll_create()时的size.
- 参数timeout是超时时间 (毫秒，0会立即返回，-1是永久阻塞).
- 如果函数调用成功，返回对应I/O上已准备好的文件描述符数目，如返回0表示已超时, 返回小于0表示函
数失败.


#### epoll的优点

- 接口使用方便: 虽然拆分成了三个函数, 但是反而使用起来更方便高效. 不需要每次循环都设置关注的文件描述符, 也做到了输入输出参数分离开

- 数据拷贝轻量: 只在合适的时候调用 EPOLL_CTL_ADD 将文件描述符结构拷贝到内核中,这个操作并不频繁(而select/poll都是每次循环都要进行拷贝)

- 事件回调机制: 避免使用遍历, 而是使用回调函数的方式,将就绪的文件描述符结构加入到就绪队列中,epoll_wait返回直接访问就绪队列就知道哪些文件描述符就绪. 这个操作时间复杂度O(1). 即使文件描述符数目很多, 效率也不会受到影响.

- 没有数量限制: 文件描述符数目无上限.

> epoll中没有使用了内存映射机制,定义的struct epoll_event是我们在用户空间中分配好的内存. 势必还是需要将内核的数据拷贝到这个用户空间的内存中


#### epoll工作方式


##### 水平触发^[LT：Level Triggered]

- 当epoll检测到socket上事件就绪的时候, 可以不立刻进行处理. 或者只处理一部分.
- 一次没有读完缓冲区数据，再第二次调用 epoll_wait 时, epoll_wait仍然会立刻返回并通知socket读事件就绪.
- 直到缓冲区上所有的数据都被处理完, epoll_wait 才不会立刻返回.
- 支持阻塞读写和非阻塞读写

##### 边缘触发^[ET：Edge Triggered]

将socket添加到epoll描述符的时候使用了EPOLLET标志, epoll进入ET工作模式

- 当epoll检测到socket上事件就绪时, 必须立刻处理.
- 第一次读完, 缓冲区还剩数据, 在第二次调用 epoll_wait 的时候,epoll_wait 不会再返回了.
- 也就是说, ET模式下, 文件描述符上的事件就绪后, 只有一次处理机会.
- ET的性能比LT性能更高( epoll_wait 返回的次数少了很多). Nginx^[Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，在BSD-like 协议下发行。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。]默认采用ET模式使用epoll.
- 只支持非阻塞的读写

> select和poll其实也是工作在LT模式下. epoll既可以支持LT, 也可以支持ET.


###### 使用 ET 模式的 epoll, 需要将文件描述设置为非阻塞. 这个不是接口上的要求, 而是 "工程实践" 上的要求.



如果服务端写的代码是阻塞式的read, 并且一次只 read部分数据的话,剩下的数据就会待在缓冲区中

由于 epoll 是ET模式, 并不会认为文件描述符读就绪. epoll\_wait 就不会再次返回. 剩下的数据会一直在缓冲区中. 

客户端要读到服务器的响应, 才会发送下一个请求,客户端发送了下一个请求, epoll_wait 才会返回, 才能去读缓冲区中剩余的数据.






#### 对比LT和ET


LT是 epoll 的默认行为. 使用 ET 能够减少 epoll 触发的次数. 但是代价就是强逼着程序猿一次响应就绪过程中就把所有的数据都处理完.
相当于一个文件描述符就绪之后, 不会反复被提示就绪, 看起来就比 LT 更高效一些. 但是在LT情况下如果也能做到每次就绪的文件描述符都立刻处理, 不让这个就绪被重复提示的话, 其实性能也是一样的

另一方面, ET 的代码复杂程度更高





#### epoll的使用场景

epoll的高性能, 是有一定的特定场景的. 如果场景选择的不适宜, epoll的性能可能适得其反


**对于多连接, 且多连接中只有一部分连接比较活跃时, 比较适合使用epoll**


例如：典型的一个需要处理上万个客户端的服务器, 例如各种互联网APP的入口服务器, 这样的服务器就很适合epoll.
如果只是系统内部, 服务器和服务器之间进行通信, 只有少数的几个连接, 这种情况下用epoll就并不合适. 具体要根
据需求和场景特点来决定使用哪种IO模型



#### epoll中的惊群问题


惊群问题又名惊群效应。简单来说就是多个进程或者线程在等待同一个事件(多个进程或者线程同时在epoll_wait监听的socket描述符)，当事件发生时，所有线程和进程都会被内核唤醒。唤醒后通常只有一个进程获得了该事件并进行处理，其他进程发现获取事件失败后又继续进入了等待状态，在一定程度上降低了系统性能。


惊群效应之所以会占用系统资源。是因为多进程/线程的唤醒，涉及到上下文切换的问题。频繁的上下文切换带来的一个问题是数据将频繁的在寄存器与运行队列中流转。极端情况下，时间更多的消耗在进程/线程的调度上，而不是执行


##### 常见的<a href="https://zhuanlan.zhihu.com/p/60966989">惊群问题</a>

Linux 下，我们常见的惊群效应发生于我们使用 accept 以及我们 select 、poll 或 epoll 等系统提供的 API 来处理我们的网络链接


###### accept 惊群

当一个请求到达时，所有进程/线程都开始 accept ，但是最终只有一个获取成功。 Linux 2.6 之后，多个进程/线程accept都会失败，Accept 的惊群问题从内核上被处理了


###### select/poll/epoll 惊群


多个进程/线程都被唤醒，但是只有一个成功 accept ，而其余的进程在 accept 的时候，都获取到了 EAGAIN 错误

###### 从内核解决惊群问题

增加一个 EPOLLEXCLUSIVE 标志位作为辅助，用户可以通过struct epoll_event的events属性增加EPOLLEXCLUSIVE，从而保证保证唤醒的进程数小于等于我们开启的进程数

###### 应用层解决(多线程)

指定工作线程处理：当有新的链接请求进来之后，由epoll_wait的线程调用accept，建立新的连接，然后交给其他工作线程处理后续的数据读写请求，这样就可以避免了由于多线程环境下的epoll_wait惊群效应问题。

###### 应用层解决(多进程)-->Nginx


Nginx 主体的思想是通过锁的形式来处理问题。我们每个进程在监听 FD 事件之前，我们先要通过 ngx_trylock_accept_mutex 去获取一个全局的锁。如果拿锁成功，那么则开始通过ngx_process_events尝试去处理事件。如果拿锁失败，则放弃本次操作。所以从某种意义上来讲，对于某一个 FD ，Nginx 同时只有一个 Worker 来处理 FD 上的事件。从而避免惊群。


#### epoll示例LT+ET


##### LT示例

###### epoll服务器

> tcp_epoll_server.hpp


```
//封装一个 Epoll 服务器, 只考虑读就绪的情况
#pragma once
#include <vector>
#include <functional>
#include <sys/epoll.h>
#include "tcp_socket.hpp"
typedef std::function<void(const std::string&, std::string* resp)> Handler;
class Epoll {
public:
	Epoll() {
		epoll_fd_ = epoll_create(10);
	}
	~Epoll() {
		close(epoll_fd_);
	}
	bool Add(const TcpSocket& sock) const {
		int fd = sock.GetFd();
		printf("[Epoll Add] fd = %d\n", fd);
		epoll_event ev;
		ev.data.fd = fd;
		ev.events = EPOLLIN;
		int ret = epoll_ctl(epoll_fd_, EPOLL_CTL_ADD, fd, &ev);
		if (ret < 0) {
			perror("epoll_ctl ADD");
			return false;
		}
		return true;
	}
	bool Del(const TcpSocket& sock) const {
		int fd = sock.GetFd();
		printf("[Epoll Del] fd = %d\n", fd);
		int ret = epoll_ctl(epoll_fd_, EPOLL_CTL_DEL, fd, NULL);
		if (ret < 0) {
			perror("epoll_ctl DEL");
			return false;
		}
		return true;
	}
	bool Wait(std::vector<TcpSocket>* output) const {
		output->clear();
		epoll_event events[1000];
		int nfds = epoll_wait(epoll_fd_, events, sizeof(events) / sizeof(events[0]), -1);
		if (nfds < 0) {
			perror("epoll_wait");
			return false;
		}
		// [注意!] 此处必须是循环到 nfds, 不能多循环
		for (int i = 0; i < nfds; ++i) {
			TcpSocket sock(events[i].data.fd);
			output->push_back(sock);
		}
		return true;
	}
private:
	int epoll_fd_;
};
class TcpEpollServer {
public:
	TcpEpollServer(const std::string& ip, uint16_t port) : ip_(ip), port_(port) {
	}
	bool Start(Handler handler) {
		// 1. 创建 socket
		TcpSocket listen_sock;
		CHECK_RET(listen_sock.Socket());
		// 2. 绑定
		CHECK_RET(listen_sock.Bind(ip_, port_));
		// 3. 监听
		CHECK_RET(listen_sock.Listen(5));
		// 4. 创建 Epoll 对象, 并将 listen_sock 加入进去
		Epoll epoll;
		epoll.Add(listen_sock);
		// 5. 进入事件循环
		for (;;) {
			// 6. 进行 epoll_wait
			std::vector<TcpSocket> output;
			if (!epoll.Wait(&output)) {
				continue;
			}
			// 7. 根据就绪的文件描述符的种类决定如何处理
			for (size_t i = 0; i < output.size(); ++i) {
				if (output[i].GetFd() == listen_sock.GetFd()) {
					// 如果是 listen_sock, 就调用 accept
					TcpSocket new_sock;
					listen_sock.Accept(&new_sock);
					epoll.Add(new_sock);
				}
				else {
					// 如果是 new_sock, 就进行一次读写
					std::string req, resp;
					bool ret = output[i].Recv(&req);
					if (!ret) {
						// [注意!!] 需要把不用的 socket 关闭
						// 先后顺序别搞反. 不过在 epoll 删除的时候其实就已经关闭 socket 了
						epoll.Del(output[i]);
						output[i].Close();
						continue;
					}
					handler(req, &resp);
					output[i].Send(resp);
				} // end for
			} // end for (;;)
		}
		return true;
	}
private:
	std::string ip_;
	uint16_t port_;
};
```

<a href="https://blog.csdn.net/qq_43808700/article/details/115870749?utm_source=app#dict_server">dict_server.c</a> 只需要将 server 对象的类型改成 TcpEpollServer 即可




##### ET示例

将上面LT 版本稍加修改即可

1. 修改 tcp_socket.hpp, 新增非阻塞读和非阻塞写接口

2. 对于 accept 返回的 new_sock 加上 EPOLLET 这样的选项

**将 listen_sock 设为 ET, 则需要非阻塞轮询的方式 accept. 否则会导致同一时刻大量的客户端同时连接的时候, 只能 accept 一次的问题**



<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=10>
<a id="asynchronism_IO"></a>

<br/><br/>


## 异步IO^[由内核在数据拷贝完成时, 通知应用程序(而信号驱动是告诉应用程序何时可以开始拷贝数据)]

IO复用还是信号驱动，我们要读取一个数据总是要发起两阶段的请求，第一阶段询问数据状态是否准备好，第二次发送recevform请求读取数据。

异步IO就是发起读数据请求之后返回，剩下的等待数据准备好，以及把数据从内核复制到用户空间的操作都由操作系统内核完成。应用只需要向内核发送一个read 请求,告诉内核它要读取数据后即刻返回；内核收到请求后会建立一个信号联系，当数据准备就绪，内核会主动把数据从内核复制到用户空间，等所有操作都完成之后，内核会发起一个通知告诉应用，请求和读取数据是异步的，这种一次性请求的模式为异步IO模型。


**应用告知内核启动某个操作，并让内核在整个操作完成之后，通知应用，这种模型与信号驱动模型的主要区别在于，信号驱动IO只是由内核通知我们合适可以开始下一个IO操作，而异步IO模型是由内核完成操作并通知我们操作什么时候完成。**


要深入理解五种IO模型可以阅读<a href="https://zhuanlan.zhihu.com/p/115912936">100%弄明白5种IO模型</a>

> 非阻塞IO，纪录锁，系统V流机制，I/O多路转接（也叫I/O多路复用）,readv和writev函数以及存储映射IO（mmap），这些统称为高级IO.

<img src="https://img-blog.csdnimg.cn/20210129183339102.png" height=50>


















</font>